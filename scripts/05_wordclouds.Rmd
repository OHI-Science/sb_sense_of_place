---
title: "05: Wordclouds"
author: "Jamie Montgomery"
output: 
  html_document:
    theme: paper
    toc: true
    toc_float: true
    toc_depth: 2
---

# Summary

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

library(RColorBrewer)
library(tidyverse)
library(wordcloud)
library(tm)
library(qdapRegex)     # Removing URLs
library(tidytext)
library(sf)
library(textdata)
library(cowplot)
library(syuzhet) # for get_nrc_sentiment
```

# Analysis

## Setup
Load data

```{r}
data  <- read_csv("../data/tweets_nature_categorized.csv")  %>%
    mutate(coords = gsub("\\)|c\\(", "", geo_coordinates)) %>%
    separate(coords, c("lat", "lon"), sep = ", ") %>%
    mutate_at(c("lon", "lat"), as.numeric) %>% 
    st_as_sf(coords = c("lon", "lat")) %>%
    st_set_crs("+init=epsg:4326")
```

```{r}
all_sb_tweets <- data %>%
  select(full_text, date, user_type)  %>% 
  st_set_geometry(NULL)
```


```{r}
all_sb_tweets$tweets_cleaned_text <- gsub("https\\S*", "", all_sb_tweets$full_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("\n", " ", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("://t.co*", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("@\\S*", "", all_sb_tweets$tweets_cleaned_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("amp", "", all_sb_tweets$tweets_cleaned_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("[\r\n]", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("[[:punct:]]", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("https?", "", all_sb_tweets$tweets_cleaned_text)

all_sb_tweets_words <-  all_sb_tweets %>%
 select(tweets_cleaned_text) %>%
 unnest_tokens(word, tweets_cleaned_text) %>%
 anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

#additional words I'm seeing that aren't in the smart list
unwanted_words <- c("im", "ive", "1", "2", "youre", "dont", "3", "4", "5", "youll")

all_sb_words <- all_sb_tweets_words %>% count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) #remove unwanted words
```

# Results 

## Wordclouds

Top 100 words for all Santa Barbara tweets

```{r}
set.seed(1234) # for reproducibility 

png("../figs/wordcloud_top_100_all_sb.png", width=4, height=3, units="in", res=300)
wordcloud(words = all_sb_words$word, freq = all_sb_words$n, min.freq = 1,           
          max.words=100, random.order=FALSE, rot.per=0.35,            
          colors=brewer.pal(8, "Dark2"))
```

Split between tourists and locals

```{r}

tourists <-  all_sb_tweets %>%
  filter(user_type == "tourist") %>%
  select(tweets_cleaned_text) %>%
  unnest_tokens(word, tweets_cleaned_text) %>%
  anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

tourist_words <- tourists %>% count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) %>% #remove unwanted words
  mutate(type = "tourist")%>%
  arrange(desc(n)) %>%
  slice(1:20)

locals <-  all_sb_tweets %>%
  filter(user_type == "local") %>%
  select(tweets_cleaned_text) %>%
  unnest_tokens(word, tweets_cleaned_text) %>%
  anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

local_words <- locals %>% count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) %>% #remove unwanted words
  mutate(type = "local") %>%
  arrange(desc(n)) %>%
  slice(1:20)

combo <- bind_rows(tourist_words, local_words)


png("../figs/top_20_words_by_user_type.png", width=4, height=3, units="in", res=300)
ggplot(combo, aes(x = reorder(word, n), y = n, fill = type)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~type, scales = "free") +
  theme_minimal() +
  theme(legend.position =  "none") +
  labs(x = "", y = "")

```


By CPAD site

```{r}
cpad <- read_sf("../data/cpad_fixed.shp")

cpad_tweets <- st_intersection(data, cpad)

sites <- unique(cpad_tweets$SITE_NAME)

out <- data.frame()

for(i in 1:length(sites)){
  
  site <- sites[i]
  df   <- cpad_tweets %>%
           filter(SITE_NAME == site) %>%
    st_set_geometry(NULL)

df$tweets_cleaned_text <- gsub("https\\S*", "", df$full_text) 
df$tweets_cleaned_text <- gsub("\n", " ", df$tweets_cleaned_text)
df$tweets_cleaned_text <- gsub("://t.co*", "", df$tweets_cleaned_text)
df$tweets_cleaned_text <- gsub("@\\S*", "", df$tweets_cleaned_text) 
df$tweets_cleaned_text <- gsub("amp", "", df$tweets_cleaned_text) 
df$tweets_cleaned_text <- gsub("[\r\n]", "", df$tweets_cleaned_text)
df$tweets_cleaned_text <- gsub("[[:punct:]]", "", df$tweets_cleaned_text)
df$tweets_cleaned_text <- gsub("https?", "", df$tweets_cleaned_text)
  
df_words <-  df %>%
 select(tweets_cleaned_text) %>%
 unnest_tokens(word, tweets_cleaned_text) %>%
 anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

df_words_10 <- df_words %>% 
  count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) %>% #remove unwanted words
  mutate(Site = site)

out <- rbind(df_words_10[1:10,], out)
}

ggplot(out, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  coord_flip() +
  theme_minimal() +
  labs(y = "Number of mentions",
       x = "",
       title = paste0("Top 10 words from ", site)) +
  facet_wrap(~Site)

```

## Sentiment analysis

### Overall sentiment of SB tweets

Most common positive/negative words across entire Santa Barbara dataset

```{r}
sb_sent <- all_sb_tweets_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

pos <- sb_sent %>%
  filter(sentiment == "positive") %>%
  arrange(desc(n)) %>%
  slice(1:10)

neg <- sb_sent %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  slice(1:10) 

p1 <- ggplot(pos, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Positive",
       x = "",
       y = "Count")
p2 <- ggplot(neg, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkred") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Negative",
       x = "",
       y = "Count")

png("../figs/top10_pos_neg_words_sb.png", width=4, height=3, units="in", res=300)
plot_grid(p1, p2)
```

Bar graph for the different emotions

```{r}
all_sb_text <- data$full_text
# mysentiment <- get_nrc_sentiment((all_sb_text))
# 
# #calculationg total score for each sentiment
# sentimentscores <- data.frame(colSums(mysentiment[,]))
# 
# names(sentimentscores) <- "Score"
# sentimentscores <- cbind("sentiment" = rownames(sentimentscores), sentimentscores)
# rownames(sentimentscores) <- NULL

#write.csv(sentimentscores, file = "../data/sentimentscores_all_sb.csv")

sentimentscores = read_csv("../data/sentimentscores_all_sb.csv")

#plotting the sentiments with scores
ggplot(data = sentimentscores, aes(x = sentiment, y = Score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  theme_minimal() +
  theme(legend.position = "none") +
  xlab("") +
  ylab("scores") +
  ggtitle("Sentiments of Santa Barbara tweets")
```

Positive/negative scores over time

```{r}
sentiment_bing <- function(twt){
  #Step 1; perform basic text cleaning (on the tweet)
  twt_tbl = tibble(text = twt) %>% 
    mutate(
      stripped_text = gsub("http\\S+", "", text)
    ) %>%
    unnest_tokens(word, stripped_text) %>%
    anti_join(stop_words) %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE) %>%
    ungroup() %>%
    ## Create a column "score", that assigns a -1 to all negative words, and 1 to all positive words
    mutate(
      score = case_when(
        sentiment == "negative" ~ n*(-1),
        sentiment == "positive" ~ n*1)
      )
  #Calculate total score
  sent_score <- case_when(
    nrow(twt_tbl) == 0 ~ 0, #if no words, score is 0
    nrow(twt_tbl) >0 ~ sum(twt_tbl$score) #otherwise, sum up pos/neg
  )
  #This is to keep track of which tweets contained no words at all from bing list
  zero.type <- case_when(
    nrow(twt_tbl) == 0 ~ "Type 1", #type 1: no words at all, zero = no
    nrow(twt_tbl) > 0 ~ "Type 2" #Type 2: zero means sum of words = 0
  )
  
  list(score = sent_score, type = zero.type, twt_tbl = twt_tbl)
}
```

2. over time

Apply the function over time and get average score. On average scores are always positive, and are growing more positive over time.

```{r, eval = F}
# dates <- unique(all_sb_tweets$date)
# out <- data.frame()
# 
# for(i in 1:length(unique(all_sb_tweets$date))){
#   print(i)
#   ymd <- dates[i]
#   sbdf <- all_sb_tweets %>%
#     filter(date == ymd)
# 
# a <- lapply(sbdf$tweets_cleaned_text, function(x){sentiment_bing(x)})
# 
# b <- tibble(
#     score = unlist(map(a, 'score')),
#     type = unlist(map(a, 'type'))) %>%
#     mutate(date = ymd,
#            avg_score = mean(score)) %>%
#   select(date, avg_score) %>%
#   distinct()
# 
# out <- bind_rows(out, b)
# }
# 
# write_csv(out, "../data/avg_score_by_date_bing.csv")
```

```{r}
avg_scores <- read_csv("../data/avg_score_by_date_bing.csv")
ggplot(avg_scores, aes(x = date, y = avg_score)) +
  geom_line() +
  geom_smooth() +
  theme_minimal()
```


3. By CPAD area over time

4. Nature-based vs non-nature based