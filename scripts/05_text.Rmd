---
title: "05: Text analysis"
author: "Jamie Montgomery"
output: 
  html_document:
    theme: paper
    toc: true
    toc_float: true
    toc_depth: 2
---

# Summary

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
library(RColorBrewer)
library(tidyverse)
library(wordcloud)
library(tm)
library(qdapRegex)     # Removing URLs
library(tidytext)
library(sf)
library(textdata)
library(cowplot)
library(syuzhet) # for get_nrc_sentiment
```

# Analysis

## Setup
Load data

```{r}
data  <- read_csv("../data/tweets_nature_categorized.csv")  %>%
    mutate(coords = gsub("\\)|c\\(", "", geo_coordinates)) %>%
    separate(coords, c("lat", "lon"), sep = ", ") %>%
    mutate_at(c("lon", "lat"), as.numeric) %>% 
    st_as_sf(coords = c("lon", "lat")) %>%
    st_set_crs("+init=epsg:4326")
```

```{r}
all_sb_tweets <- data %>%
  select(full_text, date, user_type, nature_word)  %>% 
  st_set_geometry(NULL)
```


```{r}
all_sb_tweets$tweets_cleaned_text <- gsub("https\\S*", "", all_sb_tweets$full_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("\n", " ", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("://t.co*", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("@\\S*", "", all_sb_tweets$tweets_cleaned_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("amp", "", all_sb_tweets$tweets_cleaned_text) 
all_sb_tweets$tweets_cleaned_text <- gsub("[\r\n]", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("[[:punct:]]", "", all_sb_tweets$tweets_cleaned_text)
all_sb_tweets$tweets_cleaned_text <- gsub("https?", "", all_sb_tweets$tweets_cleaned_text)

all_sb_tweets_words <-  all_sb_tweets %>%
 unnest_tokens(word, tweets_cleaned_text) %>%
 anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

#additional words I'm seeing that aren't in the smart list.
unwanted_words <- c("im", "ive", "1", "2", "youre", "dont", "3", "4", "5", "youll", "santa", "barbara", "california", "santabarbara")

all_sb_words <- all_sb_tweets_words %>% count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) #remove unwanted words
```

# Results 

## Wordclouds

Top 100 words for all Santa Barbara tweets

```{r}

png("../figs/wordcloud_top_100_all_sb.png", width=12,height=8, units='in', res=300)
par(mar = rep(0, 4))
wordcloud(words = all_sb_words$word, freq = all_sb_words$n, min.freq = 1,         
          max.words=100, random.order=FALSE, rot.per=0.35,            
          colors=brewer.pal(8, "Dark2"))
```

Most popular 20 words for tourists and locals

```{r}
tourists <-  all_sb_tweets %>%
  filter(user_type == "tourist") %>%
  select(tweets_cleaned_text) %>%
  unnest_tokens(word, tweets_cleaned_text) %>%
  anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

tourist_words <- tourists %>% 
  count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) %>% #remove unwanted words
  mutate(type = "tourist")%>%
  arrange(desc(n)) %>%
  slice(1:20)

locals <-  all_sb_tweets %>%
  filter(user_type == "local") %>%
  select(tweets_cleaned_text) %>%
  unnest_tokens(word, tweets_cleaned_text) %>%
  anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

local_words <- locals %>% 
  count(word, sort=TRUE) %>%
  filter(!word %in% unwanted_words) %>% #remove unwanted words
  mutate(type = "local") %>%
  arrange(desc(n)) %>%
  slice(1:20)

combo <- bind_rows(tourist_words, local_words)


png("../figs/top_20_words_by_user_type.png", width=4, height=3, units="in", res=300)
ggplot(combo, aes(x = reorder(word, n), y = n, fill = type)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~type, scales = "free") +
  theme_minimal() +
  theme(legend.position =  "none") +
  labs(x = "", y = "")

```

### Look at most important words for nature-based tweets using the TF-IDF method

These are the top 10 most important words in nature-based tweets. All of these words are in the dictionary, not surprisingly, which could be biasing results.

```{r tfidf}
sb_tfidf <- all_sb_tweets_words %>%
            count(word, nature_word) %>%
            bind_tf_idf(word, nature_word, n)

sb_tfidf_top <- sb_tfidf %>%
  arrange(desc(tf_idf)) %>%
  top_n(10) 

ggplot(sb_tfidf_top) +
  geom_bar(aes(x = reorder(word, tf_idf), y = tf_idf), stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(x = "", y = "TF_IDF")

ggsave("../figs/sb_nature_tweets_top_words.png")
```

We can look at this across tourists/locals (for nature-based tweets)

```{r}
user_tfidf <- all_sb_tweets_words %>%
  filter(nature_word == 1) %>% 
  count(word, user_type) %>%
  bind_tf_idf(word, user_type, n)

user_tfidf_top <- user_tfidf %>%
  group_by(user_type) %>%
  arrange(desc(tf_idf)) %>%
  top_n(10)

ggplot(user_tfidf_top) +
  geom_bar(aes(x = reorder(word, tf_idf), y = tf_idf, fill = user_type), stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(x = "", y = "TF_IDF") +
  facet_wrap(~user_type, scales = "free") +
  theme(legend.position = "none")


ggsave("../figs/tourist-local_nature_tweets_top_words.png")
```

We can also look at it across each CPAD area. This looks at all tweets from those areas.

```{r, fig.width = 10, fig.height = 6}
tweets_cpad <- read_sf("../data/tweets_in_cpad_areas.shp") %>%
  st_set_geometry(NULL) %>%
  select(fll_txt, usr_typ, ntr_wrd, SITE_NA) %>%
  unnest_tokens(word, fll_txt) %>%
  anti_join(get_stopwords(language = "en", source = "smart")) #remove stop words using the "smart" source of 571 words

#by site area 
sites_tfidf <- tweets_cpad %>%
  filter(ntr_wrd == 1) %>%
  count(word, SITE_NA) %>%
  bind_tf_idf(word, SITE_NA, n) %>%
  group_by(SITE_NA) %>%
  arrange(desc(tf_idf)) %>%
  top_n(10)

sites <- c("Santa Barbara Harbor", "Los Padres National Forest", "Stearns Wharf", "Carpinteria State Beach", "Arroyo Burro Beach County Park", "Santa Barbara Maritime Museum", "Chase Palm Park", "Leadbetter Beach", "Douglas Family Reserve", "East Beach", "Ellwood Mesa")

ggplot(sites_tfidf %>% filter(SITE_NA %in% sites)) +
  geom_bar(aes(x = reorder(word, tf_idf), y = tf_idf, fill = SITE_NA), stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(x = "", y = "TF_IDF") +
  facet_wrap(~SITE_NA, scales = "free") +
  theme(legend.position = "none")


ggsave("../figs/cpad_tweets_top_words.png")
```

Just look at all nature tweets across CPAD areas

```{r}
sites_tfidf <- tweets_cpad %>%
  count(word, ntr_wrd) %>%
  bind_tf_idf(word, ntr_wrd, n) %>%
  group_by(ntr_wrd) %>%
  arrange(desc(tf_idf)) %>%
  top_n(10)


ggplot(sites_tfidf %>% filter(ntr_wrd == 1)) +
  geom_bar(aes(x = reorder(word, tf_idf), y = tf_idf, fill = ntr_wrd), stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(x = "", y = "TF_IDF", title = "Important words from nature-based tweets in CPAD areas") +
  theme(legend.position = "none")
```

Is this different between tourists and locals?

```{r}

user_tfidf <- tweets_cpad %>%
  filter(ntr_wrd == 1) %>%
  count(word, usr_typ) %>%
  bind_tf_idf(word, usr_typ, n) %>%
  group_by(usr_typ) %>%
  arrange(desc(tf_idf)) %>%
  top_n(10)


ggplot(user_tfidf) +
  geom_bar(aes(x = reorder(word, tf_idf), y = tf_idf, fill = usr_typ), stat = "identity") +
  coord_flip() +
  theme_classic() +
  labs(x = "", y = "TF_IDF", title = "Important words from nature-based tweets in CPAD areas") +
  theme(legend.position = "none") +
  facet_wrap(~usr_typ, scales = "free")
```


----

## Sentiment analysis

Most common positive/negative words across entire Santa Barbara dataset

```{r}
sb_sent <- all_sb_tweets_words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

pos <- sb_sent %>%
  filter(sentiment == "positive") %>%
  arrange(desc(n)) %>%
  slice(1:10)

neg <- sb_sent %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  slice(1:10) 

p1 <- ggplot(pos, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Positive",
       x = "",
       y = "Count")
p2 <- ggplot(neg, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkred") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Negative",
       x = "",
       y = "Count")

png("../figs/top10_pos_neg_words_sb.png", width=4, height=3, units="in", res=300)
plot_grid(p1, p2)
```

**Tourists**

Top positive and negative words from tourists
```{r}
t_sent <-  all_sb_tweets %>%
   filter(user_type == "tourist") %>%
   select(tweets_cleaned_text) %>%
   unnest_tokens(word, tweets_cleaned_text) %>%
   anti_join(get_stopwords(language = "en", source = "smart")) %>%
   inner_join(get_sentiments("bing")) %>%
   count(word, sentiment, sort = TRUE) %>%
   ungroup()

pos <- t_sent %>%
  filter(sentiment == "positive") %>%
  arrange(desc(n)) %>%
  slice(1:10)

neg <- t_sent %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  slice(1:10) 

p1 <- ggplot(pos, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Positive",
       x = "",
       y = "Count")
p2 <- ggplot(neg, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkred") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Negative",
       x = "",
       y = "Count")

png("../figs/top10_pos_neg_words_tourists.png", width=4, height=3, units="in", res=300)
plot_grid(p1, p2)
```

**Locals**

Top positive and negative words from locals
```{r}
l_sent <- all_sb_tweets %>%
   filter(user_type == "local") %>%
   select(tweets_cleaned_text) %>%
   unnest_tokens(word, tweets_cleaned_text) %>%
   anti_join(get_stopwords(language = "en", source = "smart"))%>%
   inner_join(get_sentiments("bing")) %>%
   count(word, sentiment, sort = TRUE) %>%
   ungroup()

pos <- l_sent %>%
  filter(sentiment == "positive") %>%
  arrange(desc(n)) %>%
  slice(1:10)

neg <- l_sent %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  slice(1:10) 

p1 <- ggplot(pos, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Positive",
       x = "",
       y = "Count")
p2 <- ggplot(neg, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkred") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Negative",
       x = "",
       y = "Count")

png("../figs/top10_pos_neg_words_locals.png", width=4, height=3, units="in", res=300)
plot_grid(p1, p2)
```

### Nature based tweets

```{r}
nat_sent <- all_sb_tweets %>%
   filter(nature_word == 1) %>%
   select(tweets_cleaned_text) %>%
   unnest_tokens(word, tweets_cleaned_text) %>%
   anti_join(get_stopwords(language = "en", source = "smart")) %>%
   inner_join(get_sentiments("bing")) %>%
   count(word, sentiment, sort = TRUE) %>%
   ungroup()

pos <- nat_sent %>%
  filter(sentiment == "positive") %>%
  arrange(desc(n)) %>%
  slice(1:10)

neg <- nat_sent %>%
  filter(sentiment == "negative") %>%
  arrange(desc(n)) %>%
  slice(1:10) 

p1 <- ggplot(pos, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkblue") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Positive",
       x = "",
       y = "Count")
p2 <- ggplot(neg, aes(x = reorder(word, n), y = n)) +
  geom_col(fill = "darkred") +
  theme_minimal() +
  coord_flip() +
  labs(title = "Negative",
       x = "",
       y = "Count")

png("../figs/top10_pos_neg_words_nature.png", width=4, height=3, units="in", res=300)
plot_grid(p1, p2)
```

----

### Most common tweet emotions

```{r, eval = F}
all_sb_text <- data$full_text
mysentiment <- get_nrc_sentiment((all_sb_text))
 
 #calculationg total score for each sentiment
 sentimentscores <- data.frame(colSums(mysentiment[,]))
 
 names(sentimentscores) <- "Score"
 sentimentscores <- cbind("sentiment" = rownames(sentimentscores), sentimentscores)
 rownames(sentimentscores) <- NULL

write.csv(sentimentscores, file = "../data/sentimentscores_all_sb.csv")
```

```{r}
sentimentscores = read_csv("../data/sentimentscores_all_sb.csv")

#plotting the sentiments with scores
ggplot(data = sentimentscores, aes(x = sentiment, y = Score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  theme_minimal() +
  theme(legend.position = "none") +
  xlab("") +
  ylab("scores") +
  ggtitle("Sentiments of Santa Barbara tweets")
```

### Nature-based

```{r, eval = F}
nature_text <- data %>% filter(nature_word == 1) %>% pull(full_text)
 mysentiment <- get_nrc_sentiment((nature_text))

## calculationg total score for each sentiment
sentimentscores <- data.frame(colSums(mysentiment[,]))

names(sentimentscores) <- "Score"
sentimentscores <- cbind("sentiment" = rownames(sentimentscores), sentimentscores)
rownames(sentimentscores) <- NULL

write.csv(sentimentscores, file = "../data/sentimentscores_nature.csv")
```

```{r}
sentimentscores = read_csv("../data/sentimentscores_nature.csv")

#plotting the sentiments with scores
ggplot(data = sentimentscores, aes(x = sentiment, y = Score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  theme_minimal() +
  theme(legend.position = "none") +
  xlab("") +
  ylab("scores") +
  ggtitle("Sentiments of nature-based tweets")
```

### Tourists
```{r, eval = F}
tourist_text <- data %>% filter(user_type == "tourist") %>% pull(full_text)
 mysentiment <- get_nrc_sentiment((tourist_text))

## calculationg total score for each sentiment
sentimentscores <- data.frame(colSums(mysentiment[,]))

names(sentimentscores) <- "Score"
sentimentscores <- cbind("sentiment" = rownames(sentimentscores), sentimentscores)
rownames(sentimentscores) <- NULL

write.csv(sentimentscores, file = "../data/sentimentscores_tourist.csv")
```

```{r}
sentimentscores = read_csv("../data/sentimentscores_tourist.csv")

#plotting the sentiments with scores
ggplot(data = sentimentscores, aes(x = sentiment, y = Score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  theme_minimal() +
  theme(legend.position = "none") +
  xlab("") +
  ylab("scores") +
  ggtitle("Sentiments of tourist tweets")
```

### Locals
```{r, eval = F}
local_text <- data %>% filter(user_type == "local") %>% pull(full_text)
 mysentiment <- get_nrc_sentiment((local_text))

## calculationg total score for each sentiment
sentimentscores <- data.frame(colSums(mysentiment[,]))

names(sentimentscores) <- "Score"
sentimentscores <- cbind("sentiment" = rownames(sentimentscores), sentimentscores)
rownames(sentimentscores) <- NULL

write.csv(sentimentscores, file = "../data/sentimentscores_local.csv")
```

```{r}
sentimentscores = read_csv("../data/sentimentscores_local.csv")

#plotting the sentiments with scores
ggplot(data = sentimentscores, aes(x = sentiment, y = Score)) +
  geom_bar(aes(fill = sentiment), stat = "identity") +
  theme_minimal() +
  theme(legend.position = "none") +
  xlab("") +
  ylab("scores") +
  ggtitle("Sentiments of locals tweets")
```

### Positive/negative scores over time

```{r}
sentiment_bing <- function(twt){
  #Step 1; perform basic text cleaning (on the tweet)
  twt_tbl = tibble(text = twt) %>% 
    mutate(
      stripped_text = gsub("http\\S+", "", text)
    ) %>%
    unnest_tokens(word, stripped_text) %>%
    anti_join(stop_words) %>%
    inner_join(get_sentiments("bing")) %>%
    count(text, word, sentiment, sort = TRUE) %>%
    ungroup() %>%
    ## Create a column "score", that assigns a -1 to all negative words, and 1 to all positive words
    mutate(
      score = case_when(
        sentiment == "negative" ~ n*(-1),
        sentiment == "positive" ~ n*1)
      )
  #Calculate total score
  sent_score <- case_when(
    nrow(twt_tbl) == 0 ~ 0, #if no words, score is 0
    nrow(twt_tbl) >0 ~ sum(twt_tbl$score) #otherwise, sum up pos/neg
  )
  #This is to keep track of which tweets contained no words at all from bing list
  zero.type <- case_when(
    nrow(twt_tbl) == 0 ~ "Type 1", #type 1: no words at all, zero = no
    nrow(twt_tbl) > 0 ~ "Type 2" #Type 2: zero means sum of words = 0
  )
  
  list(score = sent_score, type = zero.type, twt_tbl = twt_tbl, text = twt)
}
```

2. over time

Apply the function over time and get average score. On average scores are always positive, and are growing more positive over time.

All sb tweet scores
```{r, eval = F}
dates <- unique(all_sb_tweets$date)
out <- data.frame()

for(i in 1:length(unique(all_sb_tweets$date))){
  print(i)
  ymd <- dates[i]
  sbdf <- all_sb_tweets %>%
    filter(date == ymd)

a <- lapply(sbdf$tweets_cleaned_text, function(x){sentiment_bing(x)})

b <- tibble(
    score = unlist(map(a, 'score')),
    type = unlist(map(a, 'type')),
    text = unlist(map(a, 'text'))) %>%
    mutate(date = ymd,
           avg_daily_score = mean(score))

out <- bind_rows(out, b)
}


#combine with twitter_data to get the nature and user type info
df <- all_sb_tweets %>%
  left_join(out, by = c("tweets_cleaned_text" = "text", "date")) %>%
  mutate(sentiment = case_when(
    score > 0 ~ "Positive",
    score < 0 ~ "Negative",
    score == 0 ~ "Neutral"))

write_csv(df, "../data/all_tweets_bing_score.csv")
```

Look at avg sentiment over time for:
1. All of SB tweets
2. SB nature vs non-nature
3. Tourist vs local
4. Tourist vs local vs nature vs non-nature

```{r}
#all of sb
all_sb <- df %>%
  select(date, avg_daily_score) %>%
  distinct() %>%
  mutate(type = "All tweets")

ggplot(all_sb, aes(x = date, y = avg_daily_score)) +
  geom_smooth(se = F) +
  theme_minimal()
```

SB nature
```{r}
#sb nature
sb_nature <- df %>%
  group_by(date, nature_word) %>%
  summarize(avg_score = mean(score)) %>%
  mutate(type = ifelse(nature_word == 1, "All SB nature tweets", "All SB non-nature tweets"))

ggplot(sb_nature, aes(x = date, y = avg_score, color = type)) +
  geom_smooth(se = F) +
  theme_minimal()
```

Tourist vs local

```{r}
#users
users <- df %>%
  group_by(date, user_type) %>%
  summarize(avg_score = mean(score)) %>%
  mutate(type = user_type)

ggplot(users, aes(x = date, y = avg_score, color = type)) +
  geom_smooth(se = F) +
  theme_minimal()
```

Tourists and locals, nature and non-nature

```{r}
#users
users <- df %>%
  group_by(date, user_type, nature_word) %>%
  summarize(avg_score = mean(score)) %>%
  mutate(type = case_when(
    nature_word == 0 & user_type == "local" ~ "Local non-nature",
    nature_word == 0 & user_type == "tourist" ~ "Tourist non-nature",
    nature_word == 1 & user_type == "local" ~ "Local nature",
    nature_word == 1 & user_type == "tourist" ~ "Tourist nature"
  ))

ggplot(users, aes(x = date, y = avg_score, color = type)) +
  geom_smooth(se = F) +
  theme_minimal()
```

## Look at dips in sentiment

Are these associated with specific events? The election? Thomas Fire?

```{r}
#need to find the dates where the sentiment starts decreasing. do this by comparing the score to the day before. Once differences become negative, we'll be able to get the date of the dips.

dips <- users %>%
  group_by(type) %>%
  arrange(date, .by_group = TRUE) %>%
  mutate(diff = avg_score - lag(avg_score, default = first(avg_score)))

ggplot(dips, aes(x = date, y = diff, color = type)) +
  geom_line()

```

