---
title: "Testing twitter data"
author: "Jamie Afflerbach"
date: "7/16/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages("rtweet")
library(tidyverse)
library(rtweet)
library(ggmap)
library(sf)
```

Trying out geolocation

Search by geo-locationâ€”for example, find 10,000 tweets in the English language sent from the United States. Note: lookup_coords() requires users have a Google API key


Request number of tweets from Santa Barbara

This API will not let us get anything previous to the last 9 days...
```{r}

## search for 100,000 tweets sent from santa barbara. This takes 
sb_tweets <- search_tweets(
    geocode = "34.424052,-119.702025,10mi", #within 10 miles of Santa Barbara Courthouse
    include_rts = FALSE, #don't include retweets
    `-filter` = "replies", #don't include replies
    retryonratelimit=TRUE, #re-query after 15 minutes
    n = 100000
)

## create lat/lng variables using all available tweet and profile geo-location data
rt <- lat_lng(sb_tweets)

## plot state boundaries
par(mar = c(0, 0, 0, 0))
maps::map("county", "california,santa barbara", lwd = .25)

## plot lat and lng points onto state map
with(rt, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```

Seems like most we can get with this geotag radius is ~50k from the past 9 days, and only 750 have geotagged info. If this holds true we could expect to get around 30,400 per year or 121,500 geotagged tweets over 4 years. 

Clean up data

```{r}
# create new df with just the tweet texts & usernames
tweet_data <- sb_tweets %>%
    select(created_at, screen_name, text, geo_coords, location, place_name,place_full_name) %>%
    mutate(coords = gsub("\\)|c\\(", "", geo_coords)) %>%
    separate(coords, c("lat", "lon"), sep = ", ") %>%
    mutate_at(c("lon", "lat"), as.numeric) %>%
    filter(!is.na(lat))
```

Identify tourists

If location is equal to either place_name or place_full_name, not a tourist
```{r}
tweet_df <- tweet_data %>%
  mutate(user_type = case_when(
    location %in% c(place_name, place_full_name) ~ "local",
    str_detect(location, "Santa Barbara") ~ "local",
    TRUE ~ "tourist"
  ))

nrow(tweet_df) #number of tweets that actually have geotag info 

write_csv(tweet_df, "~/github/sb_sense_of_place/data/geotag_sb_tweets.csv")
```

Static map

```{r}
#get basemap
mmap <- get_map(location = c(-119.9,34.38,-119.5,34.48), source = "osm")

#turn data into sf object
tweets_sf <- st_as_sf(tweet_df, coords = c("lon", "lat"), 
                 crs = 4326)

ggmap(mmap) +
    geom_sf(data = tweets_sf, 
            aes(color = user_type),
            show.legend = "point", inherit.aes = FALSE, size = 0.8) +
    coord_sf(datum = NA) +
    theme_minimal()

```

Interactive map

```{r}
library(leaflet)
typeCol <- colorFactor(c("navy", "red"), domain = c("local", "tourist"))

# plot points on top of a leaflet basemap

site_locations <- leaflet(tweet_df) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addCircleMarkers(lng = ~lon, lat = ~lat, popup = ~text,
                   radius = 3, stroke = FALSE,
                   color = ~typeCol(user_type), fillOpacity = 1)

site_locations
```

Tourist to local ratio

```{r}
l_to_t <- tweet_df %>%
  group_by(user_type) %>%
  tally() %>%
  spread(key = user_type, value = n) %>%
  mutate(ratio = local/tourist) %>%
  .$ratio

l_to_t
```

